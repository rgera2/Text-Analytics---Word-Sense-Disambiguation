{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Supervised_WSD","provenance":[],"collapsed_sections":[],"mount_file_id":"1Tnj_1bsBwTOxn_X4Zv9m7CYg2C71NPK8","authorship_tag":"ABX9TyP7gxIJH0KCdxnSrMe3fw3B"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MN4BvbkgvNCN","executionInfo":{"status":"ok","timestamp":1606946040147,"user_tz":360,"elapsed":475,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}},"outputId":"6f499b51-c454-4a87-cd8b-36ab3c8a96e5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpXXmZ27vZB8","executionInfo":{"status":"ok","timestamp":1606946041207,"user_tz":360,"elapsed":1518,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}},"outputId":"71f8cb27-0fd8-4f63-fcae-f9eb13df63b6"},"source":["# loading necessary dependies \n","import pandas as pd\n","from collections import  Counter\n","import re\n","import numpy as np\n","import math\n","import random\n","\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize\n","\n","from gensim.parsing.preprocessing import remove_stopwords\n","\n","import spacy\n","sp = spacy.load('en_core_web_sm')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZNlUDOZ9vclC","executionInfo":{"status":"ok","timestamp":1606946041208,"user_tz":360,"elapsed":1501,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}}},"source":["\"\"\"\n","Function to convert .data to pandas\n","\"\"\"\n","def file_dataframe(filename):\n","  input_string = '/content/drive/Shareddrives/Text Analytics/HW2/' + str(filename) + '.data'\n","  open_file = open(input_string, 'r')\n","  read_data = open_file.readlines()\n","\n","  df = pd.DataFrame(read_data)\n","\n","  # convert to dataFrame\n","\n","  out = pd.DataFrame(df[0].str.split('|',2).tolist(),columns=['Word','Sense','Text'])\n","\n","  return out\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnV5HCexverc","executionInfo":{"status":"ok","timestamp":1606946041209,"user_tz":360,"elapsed":1492,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}}},"source":["train_df = file_dataframe('train')\n","val_df = file_dataframe('validate')\n","test_df = file_dataframe('test')"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"TfiVfnVzwHBB","executionInfo":{"status":"ok","timestamp":1606946041210,"user_tz":360,"elapsed":1482,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}}},"source":["from nltk.stem import PorterStemmer \n","ps = PorterStemmer() \n","from nltk.tokenize import word_tokenize \n","\n","# from nltk.stem import WordNetLemmatizer \n","  \n","# lemmatizer = WordNetLemmatizer() \n","\n","def clean_text(text):\n","  filtered_sentence = remove_stopwords(text.lower())\n","\n","  res = re.sub(r'[^\\w\\s]', '', filtered_sentence)\n","  # words = word_tokenize(res)\n","  lem = sp(res)\n","  return ' '.join([w.lemma_ for w in lem if len(w) > 2]) # for words greater than 2 { example : s, dr, 16}"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vmoXQBNwVSk","executionInfo":{"status":"ok","timestamp":1606946041212,"user_tz":360,"elapsed":1474,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}}},"source":["def get_words(text,n):\n","  search_text = ' '.join(text.split()[1:])\n","  keyword = text.split()[0]\n","  before_keyword, keyword, after_keyword = search_text.partition(keyword)\n","  feature_words = ', '.join(after_keyword.split()[:n] + before_keyword.split()[-n:])\n","\n","  return feature_words"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBX4K89ovgSA","executionInfo":{"status":"ok","timestamp":1606946041529,"user_tz":360,"elapsed":1782,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}}},"source":["def main_function(train_df, val_df, test_df, lam, num_of_features):\n","\n","  val_df['Word'] = val_df['Word'].str.strip()\n","  # val_df.head(5)\n","\n","  train_df['Word'] = train_df['Word'].str.strip()\n","  # train_df.head(5)\n","  \n","  # getting prior probabilities\n","  prior_count = train_df.groupby(['Word','Sense']).size().reset_index().rename(columns={0:'count'}) # get c ( word | sense)\n","  # prior_count.head()\n","  # get word counts \n","  word_count = train_df.groupby(['Word']).size().reset_index().rename(columns={0:'total'}) # get c(word)\n","\n","  # join / merge \n","  prior_prob = prior_count.merge(word_count, how='left')\n","\n","  prior_prob['prob'] = prior_prob['count'] / prior_prob['total'] # get p(word|sense) = c ( word | sense) / c(word)\n","\n","  prior_prob = prior_prob[['Word','Sense','count','prob']]\n","\n","  prior_prob['word,sense'] = prior_prob['Word'] + prior_prob['Sense']\n","\n","  # saving prior probabilities into a  dict\n","  word_prior = pd.Series(prior_prob['prob'].values, index=tuple(prior_prob['word,sense'])).to_dict()\n","\n","  # # get words - \n","  # temp_df = prior_count\n","  # words = list(temp_df['Word'].unique())\n","\n","  # base_sense = []\n","\n","  # for e in words:\n","  #   most_sense = temp_df[temp_df['Word']==e]\n","\n","  #   most_count = most_sense['count'].max()\n","\n","  #   most_sense_baseline = most_sense[most_sense['count']==most_count]['Sense'].tolist()\n","\n","  #   base_sense.append(tuple([e, most_sense_baseline[0]]))\n","\n","  # sense_dict = {}\n","  # for am_sense in base_sense:\n","  #   sense_dict[am_sense[0].strip()] = am_sense[1]\n","\n","  # # baseline accuracy \n","  # for ind in val_df.index:\n","  #   val_df['pred'] = ''\n","  #   disamb_word = val_df['Word'][ind]\n","\n","  #   val_df['pred'] = sense_dict[disamb_word]\n","\n","  # # converting to int \n","  # val_df['Sense'] =pd.to_numeric(val_df['Sense'])\n","  # val_df['pred'] =pd.to_numeric(val_df['pred'])\n","  # val_df['match'] = np.where((val_df['Sense'] - val_df['pred'])==0,'Right', 'Wrong')\n","  \n","  # print('Baseline Validation accuracy is: ', val_df.groupby('match').pred.count()[0]/val_df.shape[0])\n","\n","  x = train_df['Word'].unique().tolist()\n","\n","  try_df = prior_count\n","\n","  df = []\n","  for idx, i in enumerate(x):\n","    try_temp = try_df.loc[try_df['Word']==i]\n","    v = try_temp['count'].max()\n","    df.append(tuple([i,try_temp.loc[try_temp['count']==v]['Sense'].tolist()[0]]))\n","\n","  new_df = pd.DataFrame.from_records(df, columns =['Word', 'pred'])\n","  # new_df.head(5)\n","\n","  val_df['Word'] = val_df['Word'].str.strip()\n","  # val_df.head(5)\n","\n","  train_df['Word'] = train_df['Word'].str.strip()\n","  # train_df.head(5)\n","  test_df1=val_df.merge(new_df,how='left',on='Word')\n","\n","  a = test_df1['Sense'].tolist()\n","  a = [int(i) for i in a]\n","  b = test_df1['pred'].tolist()\n","  b = [int(i) for i in b]\n","\n","  from sklearn.metrics import confusion_matrix\n","  cm_simple=confusion_matrix(a,b)\n","  # cm_simple\n","  accuracy = cm_simple.diagonal().sum()/sum(cm_simple).sum()*100\n","\n","  print('baseline accuracy -> ', accuracy)\n","\n","  df = train_df\n","  df['word_am'] = df['Word'].apply(lambda x : x.split('.')[0])\n","  df['clean_text']= df['Text'].apply(lambda x: clean_text(x))\n","  df['temp'] =  df['word_am'] + ' '+ df['clean_text']\n","\n","  # get vocab length \n","  vocab = df['clean_text'].str.cat()\n","  vocab_length = len(set(word_tokenize(vocab)))\n","\n","  df['feature'] = df['temp'].apply(lambda x : get_words(x,num_of_features))\n","\n","  feature_df = df[['Word','Sense','feature']] # filter out essesntial columns \n","\n","  feature_df['word_sense'] = feature_df['Word'] + feature_df['Sense'] # combining in %format -> \"word.pos sense\"\n","\n","  merge_df =  feature_df.groupby('word_sense')['feature'].apply(' '.join).reset_index() #grouping words in the same format of word.pos sense\n","\n","  merge_df['count'] = merge_df['feature'].apply(lambda x : Counter(x.split(','))) # counting word and their occurense in the \"word.pos sense\"\n","\n","  prior_count['word_sense'] = prior_count['Word'] + prior_count['Sense'] # gets word_sense count for getting p(f|sense) = c(f|sense) / c(word,sense)\n","\n","  sense_count = prior_count[['word_sense','count']]\n","\n","  feature_sense_df = sense_count.merge(merge_df, how='left', on='word_sense')\n","\n","  # add -1 lambda smoothing to get the probabilities of the (feature|sense)\n","  for ind in feature_sense_df.index:\n","    val_dict = feature_sense_df['count_y'][ind]\n","    tot = feature_sense_df['count_x'][ind]\n","    for k,v in val_dict.items():\n","      val_dict[k] = (v+lam)/(tot + (lam*vocab_length)) # add-1 lambda smoothing \n","\n","\n","  final_featureSense = feature_sense_df[['word_sense','count_y']]\n","\n","  feature_probs= pd.Series(final_featureSense['count_y'].values, index=tuple(final_featureSense['word_sense'])).to_dict()\n","\n","  val_df['clean_text'] = val_df['Text'].apply(lambda x: clean_text(x))\n","  val_df['temp'] = val_df['Word'].apply(lambda x : x.split('.')[0]) + ' ' + val_df['clean_text']\n","  val_df['temp'] = val_df['temp'].apply(lambda x : get_words(x,num_of_features))\n","\n","\n","  val_df['pred'] = ''\n","  for ind in val_df.index:\n","    r = val_df['Word'][ind].split()[0]\n","    w = val_df['temp'][ind].split(',')\n","    li = []\n","    sense_li = []\n","    final_list = []\n","    prob_dict = {}\n","    for element in list(feature_probs.keys()):\n","      if str(r) == element.split()[0]:\n","        li.append(element)\n","      \n","      for sense in li:\n","        for we in w:\n","          if we in list(feature_probs[sense].keys()):\n","            prob_dict[we] = feature_probs[sense][we]\n","          else:\n","            prob_dict[we] = lam/(lam*vocab_length)\n","      \n","        prob_mul_add = math.log(word_prior[sense])\n","        for v in prob_dict.values():\n","          prob_mul_add += math.log(v)\n","\n","        final_list.append(tuple([math.exp(prob_mul_add),sense]))\n","\n","    val_df['pred'][ind] = sorted(final_list)[0][1].split()[1]\n","\n","    # getting validation accuracy \n","  a = val_df['Sense'].tolist()\n","  a = [int(i) for i in a]\n","  b = val_df['pred'].tolist()\n","  b = [int(i) for i in b]\n","\n","  from sklearn.metrics import confusion_matrix\n","  cm_simple=confusion_matrix(a,b)\n","  # cm_simple\n","  accuracy = cm_simple.diagonal().sum()/sum(cm_simple).sum()*100\n","\n","  print('For features : ' + str(num_of_features) + '  and lambda: '  + str(lam))\n","\n","  print('Validation accuracy is: '+ str(accuracy) )\n","\n","  return word_prior, vocab_length, feature_probs"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziyoGvyJxFxR"},"source":["Checking value"]},{"cell_type":"code","metadata":{"id":"vkuVBkQrxDbj","executionInfo":{"status":"ok","timestamp":1606946041531,"user_tz":360,"elapsed":1776,"user":{"displayName":"Babandeep Singh","photoUrl":"","userId":"14171741079947983888"}}},"source":["# word_prior, vocab_length, feature_probs = main_function(train_df, val_df, test_df, 1,1)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgE7W22Tzb9R"},"source":["for different lambda and different num of features "]},{"cell_type":"code","metadata":{"id":"lVENwBtnzbWk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efb99f0f-bb4a-493f-fbad-4be2ccad5521"},"source":["for lam in [1,0.1,0.0001]:\n","  for num_of_features in [1,3,7]:\n","    word_prior, vocab_length, feature_probs = main_function(train_df, val_df, test_df, lam,num_of_features)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["baseline accuracy ->  81.13612004287245\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MZAo4XZYy4Ks"},"source":["# working on test data saving it. "]},{"cell_type":"code","metadata":{"id":"JbQ6fE0tyylB"},"source":["def save_test(test_df, word_prior, vocab_length, feature_probs, best_lam, best_num_of_features):\n","  # clean_text \n","  test_df['clean_text'] = test_df['Text'].apply(lambda x: clean_text(x))\n","  test_df['temp'] = test_df['Word'].apply(lambda x : x.split('.')[0]) + ' ' + test_df['clean_text']\n","  test_df['temp'] = test_df['temp'].apply(lambda x : get_words(x,best_num_of_features))\n","\n","  test_df['pred'] = ''\n","  for ind in test_df.index:\n","    r = test_df['Word'][ind].split()[0]\n","    w = test_df['temp'][ind].split(',')\n","    li = []\n","    sense_li = []\n","    final_list = []\n","    prob_dict = {}\n","    for element in list(feature_probs.keys()):\n","      if str(r) == element.split()[0]:\n","        li.append(element)\n","      \n","      for sense in li:\n","        for we in w:\n","          if we in list(feature_probs[sense].keys()):\n","            prob_dict[we] = feature_probs[sense][we]\n","          else:\n","            prob_dict[we] = best_lam/ (best_lam * vocab_length)\n","      \n","        prob_mul_add = 0\n","        for v in prob_dict.values():\n","          prob_mul_add += math.log(v)\n","\n","        final_list.append(tuple([math.exp(prob_mul_add),sense]))\n","\n","    test_df['pred'][ind] = sorted(final_list)[0][1].split()[1]\n","\n","    final_data = test_df[['Word','pred','Text']]\n","\n","  text_file = open(\"WSD-test.data\", \"w\")\n","  for ind in final_data.index:\n","    line = final_data['Word'][ind] + ' | ' + final_data['pred'][ind]+ ' | ' + final_data['Text'][ind] \n","    text_file.write(line)\n","\n","  text_file.close()\n","\n","  print('done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWOckgHXSZWs"},"source":["best_lam = 0.1\n","best_num_of_features = 3\n","save_test(test_df, word_prior, vocab_length, feature_probs, best_lam, best_num_of_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9zff6E7SiJ5"},"source":["input_string = '/content/sample.data'\n","open_file = open(input_string, 'r')\n","read_data = open_file.readlines()\n","\n","df = pd.DataFrame(read_data)\n","\n","# convert to dataFrame\n","\n","out = pd.DataFrame(df[0].str.split('|',2).tolist(),columns=['Word','Sense','Text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZmVfHVNzVES1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4xKBo2VVGEz"},"source":[""],"execution_count":null,"outputs":[]}]}